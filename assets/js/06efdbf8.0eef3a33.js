"use strict";(self.webpackChunkaurorahelp=self.webpackChunkaurorahelp||[]).push([[7195],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=c(n),m=r,h=d["".concat(l,".").concat(m)]||d[m]||p[m]||i;return n?a.createElement(h,o(o({ref:t},u),{},{components:n})):a.createElement(h,o({ref:t},u))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},7070:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var a=n(7462),r=(n(7294),n(3905));const i={authors:"Hunter",tags:["AuroraPrime Create","GenAI","AI hallucination"]},o="Addressing AI Hallucinations in Medical Writing",s={permalink:"/blog/2024/09/29/ The Cure for AI Hallucination",source:"@site/blog/2024-09-29 The Cure for AI Hallucination.md",title:"Addressing AI Hallucinations in Medical Writing",description:'In the rapidly evolving landscape of artificial intelligence, the term "hallucination" has taken on a new meaning. When prompted about topics they don\u2019t know, large language models (LLMs) tend to generate responses that may appear authoritative and plausible to end users, yet they may be completely incorrect or contain serious errors. This phenomenon, known as "hallucination," is a common bias inherent in LLMs. In sectors such as Life Sciences, where accuracy is paramount, addressing this issue is critical.',date:"2024-09-29T00:00:00.000Z",formattedDate:"September 29, 2024",tags:[{label:"AuroraPrime Create",permalink:"/blog/tags/aurora-prime-create"},{label:"GenAI",permalink:"/blog/tags/gen-ai"},{label:"AI hallucination",permalink:"/blog/tags/ai-hallucination"}],readingTime:3.62,hasTruncateMarker:!1,authors:[{name:"Hunter Shen",title:"generative AI evangelist",imageURL:"https://video.myaurora.cn/adhoc/huntershen.jpeg",key:"Hunter"}],frontMatter:{authors:"Hunter",tags:["AuroraPrime Create","GenAI","AI hallucination"]},nextItem:{title:"The Latest News on GenAI in Clinical Trials",permalink:"/blog/2024/09/27/ The Latest News on GenAI in Clinical Trials"}},l={authorsImageUrls:[void 0]},c=[{value:"Understanding AI Hallucination",id:"understanding-ai-hallucination",level:2},{value:"AuroraPrime Create: The Solution",id:"auroraprime-create-the-solution",level:2},{value:"Retrieval Augmented Generation (RAG)",id:"retrieval-augmented-generation-rag",level:3},{value:"Advanced Prompt Engineering and Customization",id:"advanced-prompt-engineering-and-customization",level:3},{value:"Reuse and Repurpose of Existing Content",id:"reuse-and-repurpose-of-existing-content",level:3},{value:"Content Quality Control (QC) Mechanism",id:"content-quality-control-qc-mechanism",level:3},{value:"The Future of Medical Writing",id:"the-future-of-medical-writing",level:2}],u={toc:c};function d(e){let{components:t,...i}=e;return(0,r.kt)("wrapper",(0,a.Z)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(6301).Z,width:"1024",height:"585"})),(0,r.kt)("p",null,'In the rapidly evolving landscape of artificial intelligence, the term "hallucination" has taken on a new meaning. When prompted about topics they don\u2019t know, large language models (LLMs) tend to generate responses that may appear authoritative and plausible to end users, yet they may be completely incorrect or contain serious errors. This phenomenon, known as "hallucination," is a common bias inherent in LLMs. In sectors such as Life Sciences, where accuracy is paramount, addressing this issue is critical.'),(0,r.kt)("p",null,"Meet AuroraPrime Create by AlphaLife Sciences, an AI-powered medical writing software designed to mitigate these risks and ensure the highest level of accuracy in medical documentation."),(0,r.kt)("h2",{id:"understanding-ai-hallucination"},"Understanding AI Hallucination"),(0,r.kt)("p",null,"AI hallucination occurs when an AI algorithm produces results that are systemically prejudiced due to erroneous assumptions in the machine learning process. This can lead to the generation of incorrect or misleading information, which is particularly dangerous in the medical field. For instance, an AI might generate a plausible-sounding but incorrect dosage recommendation for a medication, potentially putting patients at risk."),(0,r.kt)("p",null,"One way to mitigate this phenomenon is through Retrieval Augmented Generation (RAG). RAG involves automatically retrieving documents in a knowledge base that are most likely to contain elements of the response and adding them to the prompt so that the LLM has more context to give a correct answer. This technique, along with fine-tuning and other approaches, can significantly reduce the risk of hallucination."),(0,r.kt)("h2",{id:"auroraprime-create-the-solution"},"AuroraPrime Create: The Solution"),(0,r.kt)("p",null,"AuroraPrime Create leverages advanced AI technologies to provide a robust solution to the problem of AI hallucination. Here\u2019s how:"),(0,r.kt)("h3",{id:"retrieval-augmented-generation-rag"},"Retrieval Augmented Generation (RAG)"),(0,r.kt)("p",null,"AuroraPrime Create employs RAG to enhance the accuracy of its outputs. By storing both internal and external study-related documents in a corporate knowledge base, the software enhances the LLM\u2019s internal information representation. This ensures that the generated content is not only coherent but also factually accurate."),(0,r.kt)("p",null,"For example, when generating a Clinical Study Report (CSR), AuroraPrime Create automatically retrieves relevant study protocols, statistical analysis plans, and previous reports. This ensures that the CSR is consistent with existing documents and free from hallucinations."),(0,r.kt)("h3",{id:"advanced-prompt-engineering-and-customization"},"Advanced Prompt Engineering and Customization"),(0,r.kt)("p",null,"AuroraPrime Create allows for advanced prompt engineering based on specific organizational needs. This customization ensures that the AI model is aligned with the unique requirements and standards of the organization, further reducing the risk of hallucination."),(0,r.kt)("p",null,"A pharmaceutical company can leverage custom prompts alongside its corporate knowledge base to generate content that aligns with specific guidelines for document structure and style. This approach ensures all generated documents adhere to the company\u2019s standards and regulatory requirements."),(0,r.kt)("h3",{id:"reuse-and-repurpose-of-existing-content"},"Reuse and Repurpose of Existing Content"),(0,r.kt)("p",null,"AuroraPrime Create enables users to reuse and repurpose existing content from other documents. This not only maximizes productivity and consistency but also minimizes the chances of introducing errors or hallucinations."),(0,r.kt)("p",null,"When drafting a Clinical Study Report (CSR), for example, AuroraPrime Create can pull relevant sections from previously approved documents, ensuring consistency and accuracy across all documents."),(0,r.kt)("h3",{id:"content-quality-control-qc-mechanism"},"Content Quality Control (QC) Mechanism"),(0,r.kt)("p",null,"AuroraPrime Create has in place an iterative process that ensures continuous improvement and delivers high-quality AI-generated content tailored to specific requirements.The AI content quality control process involves multiple steps to ensure that the AI-generated output meets a high standard of accuracy, consistency, and relevance:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Run Test Cases:")," Execute various model-prompt combinations to explore content generation."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Evaluate")," ",(0,r.kt)("strong",{parentName:"li"},"Output"),": Compare generated content against a golden dataset to assess accuracy and quality."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Inspect Results:")," Visualize and review output to identify areas for improvement."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Fine-tune Prompts:")," Adjust prompts based on evaluation, then re-run test cases."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Iterate:")," Repeat until optimal content quality is achieved.")),(0,r.kt)("h2",{id:"the-future-of-medical-writing"},"The Future of Medical Writing"),(0,r.kt)("p",null,"The implementation of AuroraPrime Create by AlphaLife Sciences represents a significant step forward in the field of medical writing. By addressing the issue of AI hallucination at its source, AuroraPrime Create ensures that medical documents are accurate, consistent, and reliable. This not only enhances the quality of medical research but also safeguards patient safety."),(0,r.kt)("p",null,"In conclusion, the cure for AI hallucination lies in leveraging advanced technologies like Retrieval Augmented Generation, advanced prompt engineering, content reuse/repurpose, and content quality control. AuroraPrime Create embodies these solutions, setting a new standard for AI-powered medical writing. As the life sciences industry continues to evolve, tools like AuroraPrime Create will play an increasingly vital role in ensuring the accuracy and reliability of clinical documentation."))}d.isMDXComponent=!0},6301:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/image_20240929190968-5fd14883823338440c43f2ce33008595.png"}}]);